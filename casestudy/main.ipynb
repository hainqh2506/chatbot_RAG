{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1: tien xu ly file pdf txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý file PDF: D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023.pdf\n",
      "Đã xử lý và lưu file thành công vào D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023.txt\n",
      "Đang xử lý file PDF: D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023.pdf\n",
      "Đã xử lý và lưu file thành công vào D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "def format_text(text: str) -> str:\n",
    "    \"\"\"Định dạng lại văn bản theo quy tắc mong muốn.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    formatted_lines = []\n",
    "    current_paragraph = []\n",
    "    previous_line_numbered = False\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Loại bỏ các dòng trống liên tiếp\n",
    "        if not stripped_line:\n",
    "            if current_paragraph:  # Nếu có đoạn văn, kết thúc đoạn văn hiện tại\n",
    "                formatted_lines.append(' '.join(current_paragraph))\n",
    "                current_paragraph = []\n",
    "            continue  # Bỏ qua dòng trống\n",
    "\n",
    "        # Xử lý các dòng bắt đầu bằng số hoặc ký tự a), b), c)...\n",
    "        if re.match(r'^(\\d+\\.|\\w\\))', stripped_line):\n",
    "            if current_paragraph:\n",
    "                formatted_lines.append(' '.join(current_paragraph))\n",
    "                current_paragraph = []\n",
    "            current_paragraph.append(stripped_line)\n",
    "            previous_line_numbered = True\n",
    "        # Xử lý các dòng bắt đầu bằng \"Điều\"\n",
    "        elif stripped_line.startswith('Điều'):\n",
    "            if current_paragraph:\n",
    "                formatted_lines.append(' '.join(current_paragraph))\n",
    "                current_paragraph = []\n",
    "            formatted_lines.append(stripped_line)\n",
    "            previous_line_numbered = False\n",
    "        # Xử lý các dòng còn lại\n",
    "        elif stripped_line:\n",
    "            if previous_line_numbered and not stripped_line[0].isupper():\n",
    "                if current_paragraph:  # Kiểm tra nếu current_paragraph có phần tử\n",
    "                    current_paragraph[-1] += ' ' + stripped_line\n",
    "                else:\n",
    "                    current_paragraph.append(stripped_line)  # Nếu không có phần tử, thêm vào\n",
    "            else:\n",
    "                current_paragraph.append(stripped_line)\n",
    "            previous_line_numbered = False\n",
    "\n",
    "    # Nếu còn đoạn văn chưa được thêm vào\n",
    "    if current_paragraph:\n",
    "        formatted_lines.append(' '.join(current_paragraph))\n",
    "\n",
    "    # Kết hợp các đoạn văn thành văn bản\n",
    "    result_text = '\\n'.join(formatted_lines)\n",
    "\n",
    "    # Loại bỏ các dòng trống giữa các đoạn văn\n",
    "    result_text = re.sub(r'\\n+', '\\n', result_text)\n",
    "\n",
    "    return result_text\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Trích xuất văn bản từ file PDF.\"\"\"\n",
    "    try:\n",
    "        elements = partition_pdf(pdf_path, strategy=\"fast\")\n",
    "        return '\\n'.join(e.text if hasattr(e, \"text\") else e for e in elements)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Lỗi khi trích xuất text từ PDF: {e}\")\n",
    "\n",
    "def process_pdf(pdf_path: str, output_folder: str) -> None:\n",
    "    \"\"\"Xử lý và lưu nội dung tệp PDF đã định dạng.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"Không tìm thấy file PDF: {pdf_path}\")\n",
    "\n",
    "        print(f\"Đang xử lý file PDF: {pdf_path}\")\n",
    "        extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        formatted_text = format_text(extracted_text)\n",
    "        \n",
    "        # Lấy tên file PDF (không bao gồm phần mở rộng)\n",
    "        pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        output_path = os.path.join(output_folder, f\"{pdf_filename}.txt\")\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(formatted_text)\n",
    "        \n",
    "        print(f\"Đã xử lý và lưu file thành công vào {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình xử lý PDF: {e}\")\n",
    "\n",
    "def main():\n",
    "    pdf_folder = r\"D:\\DATN\\QA_System\\casestudy\\pdf\"\n",
    "    output_folder = r\"D:\\DATN\\QA_System\\casestudy\\pdf\"\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            process_pdf(pdf_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2. load and chunking + raptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã load dữ liệu từ file: D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023.txt\n",
      "Initializing RaptorPipeline...\n",
      "Initializing Vietnamese embedding model: dangvantuan/vietnamese-embedding\n",
      "Embedding model loaded.\n",
      "Summarization model loaded.\n",
      "Tokenizer loaded.\n",
      "Starting recursive_embed_cluster_summarize for level 1...\n",
      "Processing base level (level 0)...\n",
      "Starting embed_cluster_summarize for level 0...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters obtained. Number of clusters: 5\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 0): 100%|██████████| 5/5 [00:31<00:00,  6.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 0.\n",
      "Base level processing completed.\n",
      "Processing level 1...\n",
      "Starting embed_cluster_summarize for level 1...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters obtained. Number of clusters: 5\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 1): 100%|██████████| 5/5 [00:30<00:00,  6.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 1.\n",
      "Level 1 processing completed.\n",
      "Recursing to level 2...\n",
      "Starting recursive_embed_cluster_summarize for level 2...\n",
      "Processing level 2...\n",
      "Starting embed_cluster_summarize for level 2...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 5\n",
      "Clusters obtained. Number of clusters: 1\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 2): 100%|██████████| 1/1 [00:06<00:00,  6.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 2.\n",
      "Level 2 processing completed.\n",
      "Finished recursive_embed_cluster_summarize for level 2.\n",
      "Recursion to level 2 completed.\n",
      "Finished recursive_embed_cluster_summarize for level 1.\n",
      "Building final dataframe...\n",
      "Processing chunks (level 0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 24/24 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summaries (levels > 0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing summary levels: 100%|██████████| 2/2 [00:00<00:00, 121.94it/s]\n",
      "Processing files:  50%|█████     | 1/2 [01:29<01:29, 89.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Final dataframe built.\n",
      "Đã lưu kết quả RAPTOR cho file D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023.txt vào D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023_results.csv\n",
      "Đã load dữ liệu từ file: D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023.txt\n",
      "Initializing RaptorPipeline...\n",
      "Embedding model loaded.\n",
      "Summarization model loaded.\n",
      "Tokenizer loaded.\n",
      "Starting recursive_embed_cluster_summarize for level 1...\n",
      "Processing base level (level 0)...\n",
      "Starting embed_cluster_summarize for level 0...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters obtained. Number of clusters: 8\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 0): 100%|██████████| 8/8 [00:49<00:00,  6.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 0.\n",
      "Base level processing completed.\n",
      "Processing level 1...\n",
      "Starting embed_cluster_summarize for level 1...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters obtained. Number of clusters: 8\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 1): 100%|██████████| 8/8 [00:48<00:00,  6.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 1.\n",
      "Level 1 processing completed.\n",
      "Recursing to level 2...\n",
      "Starting recursive_embed_cluster_summarize for level 2...\n",
      "Processing level 2...\n",
      "Starting embed_cluster_summarize for level 2...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 8\n",
      "Clusters obtained. Number of clusters: 1\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 2): 100%|██████████| 1/1 [00:07<00:00,  7.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 2.\n",
      "Level 2 processing completed.\n",
      "Finished recursive_embed_cluster_summarize for level 2.\n",
      "Recursion to level 2 completed.\n",
      "Finished recursive_embed_cluster_summarize for level 1.\n",
      "Building final dataframe...\n",
      "Processing chunks (level 0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 37/37 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summaries (levels > 0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing summary levels: 100%|██████████| 2/2 [00:00<00:00, 81.98it/s]\n",
      "Processing files: 100%|██████████| 2/2 [03:26<00:00, 103.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Final dataframe built.\n",
      "Đã lưu kết quả RAPTOR cho file D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023.txt vào D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from data_ingestion import TXTProcessor  # step1_loaddata\n",
    "from chunking import text_splitter  # step2_chunking\n",
    "from raptor import RaptorPipeline  # step3_RAPTOR\n",
    "from utils import convert_df_to_documents\n",
    "from model_config import VietnameseEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import json\n",
    "# Các bước pipeline\n",
    "def step1_load_data(txt_file: str) -> List:\n",
    "    \"\"\"\n",
    "    Load dữ liệu từ một file .txt cụ thể.\n",
    "    \"\"\"\n",
    "    processor = TXTProcessor()\n",
    "    documents = processor.setup_txt(txt_file)\n",
    "    print(f\"Đã load dữ liệu từ file: {txt_file}\")\n",
    "    return documents\n",
    "\n",
    "def step2_chunking(documents):\n",
    "    \"\"\"\n",
    "    Chunk tài liệu thành các phần nhỏ hơn.\n",
    "    \"\"\"\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"id\"] = str(i)\n",
    "    chunks_metadata = [chunk.metadata for chunk in chunks]\n",
    "    chunks_content = [chunk.page_content for chunk in chunks]\n",
    "    return chunks_metadata, chunks_content\n",
    "\n",
    "def step3_RAPTOR(chunks_metadata, chunks_content):\n",
    "    \"\"\"\n",
    "    Thực hiện RAPTOR pipeline trên chunks.\n",
    "    \"\"\"\n",
    "    raptor = RaptorPipeline()\n",
    "    results = raptor.recursive_embed_cluster_summarize(chunks_content, chunks_metadata, level=1, n_levels=3)\n",
    "    final_df = raptor.build_final_dataframe(results)\n",
    "    return final_df\n",
    "\n",
    "def process_directory(directory: str):\n",
    "    \"\"\"\n",
    "    Đọc và xử lý toàn bộ các file trong thư mục với RAPTOR.\n",
    "    \"\"\"\n",
    "    processor = TXTProcessor(directory=directory)\n",
    "    txt_files = processor.get_txt_files()\n",
    "    \n",
    "    for txt_file in tqdm(txt_files, desc=\"Processing files\"):\n",
    "        # Bước 1: Load dữ liệu\n",
    "        documents = step1_load_data(txt_file)\n",
    "        \n",
    "        # Bước 2: Chunking\n",
    "        chunks_metadata, chunks_content = step2_chunking(documents)\n",
    "        \n",
    "        # Bước 3: RAPTOR\n",
    "        final_df = step3_RAPTOR(chunks_metadata, chunks_content)\n",
    "        \n",
    "        # Lưu kết quả thành file CSV\n",
    "        output_csv = f\"{os.path.splitext(txt_file)[0]}_results.csv\"\n",
    "        final_df[\"metadata\"] = final_df[\"metadata\"].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "        final_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Đã lưu kết quả RAPTOR cho file {txt_file} vào {output_csv}\")\n",
    "        # # Lưu kết quả thành file Pickle\n",
    "        # output_pkl = f\"{os.path.splitext(txt_file)[0]}_results.pkl\"\n",
    "        # final_df.to_pickle(output_pkl)  # Lưu DataFrame dưới dạng Pickle\n",
    "        # print(f\"Đã lưu kết quả RAPTOR cho file {txt_file} vào {output_pkl}\")\n",
    "# Thực thi pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = r\"D:\\DATN\\QA_System\\casestudy\\pdf\"  # Thay bằng đường dẫn thực tế\n",
    "    process_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ghep thanh 1 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tất cả các tệp CSV đã được ghép vào D:\\DATN\\QA_System\\casestudy\\pdf\\merged_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_csv_files(input_directory: str, output_file: str):\n",
    "    # Tạo danh sách tất cả các tệp CSV trong thư mục\n",
    "    csv_files = glob.glob(os.path.join(input_directory, \"*.csv\"))\n",
    "    \n",
    "    # Đọc và ghép tất cả các tệp CSV lại thành một DataFrame\n",
    "    df_list = []  # Danh sách để lưu các DataFrame\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)  # Đọc tệp CSV\n",
    "        df_list.append(df)  # Thêm DataFrame vào danh sách\n",
    "    \n",
    "    # Ghép tất cả các DataFrame lại thành một\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)  # ignore_index để không giữ chỉ số ban đầu\n",
    "    print(f\"Tất cả các tệp CSV đã được ghép vào {output_file}\")\n",
    "    # Lưu DataFrame đã ghép vào tệp CSV mới\n",
    "    df = merged_df\n",
    "    df.head()\n",
    "    # Lưu lại DataFrame vào excel mới\n",
    "    df.to_csv(output_file, index=False ,encoding='utf-8') \n",
    "    df.to_excel(execfile, index=False, engine=\"openpyxl\")\n",
    "    \n",
    "\n",
    "# Sử dụng hàm\n",
    "input_directory = r\"D:\\DATN\\QA_System\\casestudy\\pdf\"  # Thư mục chứa các tệp CSV\n",
    "output_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\merged_output.csv\"  # Tên tệp CSV đầu ra\n",
    "execfile = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\df.xlsx\"\n",
    "merge_csv_files(input_directory, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chuyen sang json de luu vao db cho nhanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "execfile = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\finaldf.xlsx\"\n",
    "df = pd.read_excel(execfile)\n",
    "import json\n",
    "\n",
    "# Chuyển đổi cột metadata từ chuỗi JSON thành dictionary (nếu cần)\n",
    "if isinstance(df[\"metadata\"].iloc[0], str):  # Kiểm tra kiểu dữ liệu của phần tử đầu tiên trong cột metadata\n",
    "    df[\"metadata\"] = df[\"metadata\"].apply(lambda x: json.loads(x))\n",
    "\n",
    "# Chuyển đổi DataFrame thành danh sách các dictionary\n",
    "json_data = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Lưu danh sách các dictionary vào file JSON\n",
    "output_json_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\finaldf.json\"  # Đường dẫn đến file JSON đầu ra\n",
    "with open(output_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Đã lưu dữ liệu vào file JSON: {output_json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loc theo level neu muon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lọc và lưu dữ liệu vào file D:\\DATN\\QA_System\\casestudy\\pdf\\data0.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def filter_json_by_level(input_file, output_file, target_level=0):\n",
    "    \"\"\"\n",
    "    Lọc các phần tử trong file JSON dựa trên giá trị của 'level'.\n",
    "\n",
    "    Args:\n",
    "        input_file: Đường dẫn đến file JSON đầu vào.\n",
    "        output_file: Đường dẫn đến file JSON đầu ra (sẽ ghi đè lên file đầu vào nếu giống nhau).\n",
    "        target_level: Giá trị 'level' cần giữ lại.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file {input_file}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Lỗi: File {input_file} không đúng định dạng JSON.\")\n",
    "        return\n",
    "\n",
    "    filtered_data = [\n",
    "        item for item in data\n",
    "        if \"level\" in item and item[\"level\"] == target_level\n",
    "    ]\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Đã lọc và lưu dữ liệu vào file {output_file}\")\n",
    "\n",
    "# Sử dụng hàm:\n",
    "input_json_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\data.json\"\n",
    "output_json_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\data0.json\"\n",
    "filter_json_by_level(input_json_file, output_json_file, target_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Thay 'your_token_here' bằng token của bạn\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"dangvantuan/vietnamese-embedding\")\n",
    "\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Vietnamese embedding model: dangvantuan/vietnamese-embedding\n",
      "Processing document 0\n",
      "Processing document 1\n",
      "Total documents truncated: 0\n",
      "Indexing completed. Time taken: 0.55 seconds\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "from time import time\n",
    "es = Elasticsearch(  #new 05/02/25\n",
    "    \"https://my-elasticsearch-project-acc1b0.es.ap-southeast-1.aws.elastic.cloud:443\",\n",
    "    api_key=\"SFJsUTFKUUJrYVpaM1NIdzdxNlo6ei1NVlA3azZTTy1RbllvMzBEY0kxUQ==\"\n",
    ")\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     \"https://my-elasticsearch-project-fc9fd1.es.ap-southeast-1.aws.elastic.cloud:443\",\n",
    "#     api_key=\"aXY4NkpKUUJaNVNfUEdnZHdVZ186MExmVk1iclZRWWlrS1hpeDRhOWRGUQ==\"\n",
    "# )\n",
    "\n",
    "from model_config import load_embedding_model_VN2, load_tokenizer2 , VietnameseEmbeddings\n",
    "embeddings = load_embedding_model_VN2()\n",
    "tokenizer = load_tokenizer2()\n",
    "json_file_path = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\faq_data.json\"\n",
    "index_name = \"faq_data\"\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"answer\": {\"type\": \"text\"},\n",
    "            \"category\": {\"type\": \"keyword\"},\n",
    "            \"vector\": {\"type\": \"dense_vector\", \"dims\": 768, \"similarity\": \"cosine\", \"index\": True}  # Số chiều của vector embedding\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "def generate_actions(documents):\n",
    "    truncated = 0\n",
    "    for i, record in enumerate(documents):\n",
    "        try:\n",
    "            # Get tokens\n",
    "            tokens = tokenizer.encode(record[\"question\"])\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if len(tokens) > 254:\n",
    "                token = tokens[:236]  # Chỉ lấy 254 token đầu tiên\n",
    "                record[\"question\"] = tokenizer.decode(token)  # Chuyển lại thành text\n",
    "                truncated += 1\n",
    "                print(f\"Truncated document {i} from {len(tokens)} tokens to 236 tokens\")\n",
    "                \n",
    "            embedding = embeddings.embed_query(record[\"question\"])\n",
    "            print(f\"Processing document {i}\")\n",
    "            yield {\n",
    "                    \"_op_type\": \"index\",  # Chỉ mục bản ghi\n",
    "                    \"_index\": index_name,\n",
    "                    \"_source\": {\n",
    "                        \"question\": record[\"question\"],\n",
    "                        \"answer\": record[\"answer\"],\n",
    "                        \"category\": record[\"category\"],\n",
    "                        \"vector\": embedding\n",
    "                    }\n",
    "    \n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {i}: {str(e)}\")\n",
    "            raise e\n",
    "    print(f\"Total documents truncated: {truncated}\")\n",
    "\n",
    "\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "start_time = time()\n",
    "helpers.bulk(es, generate_actions(json_data))\n",
    "print(f\"Indexing completed. Time taken: {time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 0\n",
      "Processing document 1\n",
      "Processing document 2\n",
      "Processing document 3\n",
      "Processing document 4\n",
      "Processing document 5\n",
      "Processing document 6\n",
      "Processing document 7\n",
      "Processing document 8\n",
      "Truncated document 9 from 498 tokens to 236 tokens\n",
      "Processing document 9\n",
      "Processing document 10\n",
      "Processing document 11\n",
      "Processing document 12\n",
      "Processing document 13\n",
      "Processing document 14\n",
      "Processing document 15\n",
      "Processing document 16\n",
      "Processing document 17\n",
      "Processing document 18\n",
      "Processing document 19\n",
      "Processing document 20\n",
      "Processing document 21\n",
      "Processing document 22\n",
      "Processing document 23\n",
      "Processing document 24\n",
      "Processing document 25\n",
      "Processing document 26\n",
      "Processing document 27\n",
      "Processing document 28\n",
      "Processing document 29\n",
      "Processing document 30\n",
      "Truncated document 31 from 280 tokens to 236 tokens\n",
      "Processing document 31\n",
      "Processing document 32\n",
      "Processing document 33\n",
      "Processing document 34\n",
      "Processing document 35\n",
      "Processing document 36\n",
      "Processing document 37\n",
      "Processing document 38\n",
      "Processing document 39\n",
      "Processing document 40\n",
      "Processing document 41\n",
      "Truncated document 42 from 328 tokens to 236 tokens\n",
      "Processing document 42\n",
      "Processing document 43\n",
      "Processing document 44\n",
      "Processing document 45\n",
      "Processing document 46\n",
      "Truncated document 47 from 314 tokens to 236 tokens\n",
      "Processing document 47\n",
      "Processing document 48\n",
      "Processing document 49\n",
      "Processing document 50\n",
      "Processing document 51\n",
      "Processing document 52\n",
      "Processing document 53\n",
      "Processing document 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 55\n",
      "Processing document 56\n",
      "Truncated document 57 from 523 tokens to 236 tokens\n",
      "Processing document 57\n",
      "Processing document 58\n",
      "Processing document 59\n",
      "Processing document 60\n",
      "Processing document 61\n",
      "Processing document 62\n",
      "Truncated document 63 from 295 tokens to 236 tokens\n",
      "Processing document 63\n",
      "Processing document 64\n",
      "Processing document 65\n",
      "Processing document 66\n",
      "Truncated document 67 from 305 tokens to 236 tokens\n",
      "Processing document 67\n",
      "Processing document 68\n",
      "Processing document 69\n",
      "Processing document 70\n",
      "Processing document 71\n",
      "Processing document 72\n",
      "Processing document 73\n",
      "Processing document 74\n",
      "Processing document 75\n",
      "Processing document 76\n",
      "Processing document 77\n",
      "Processing document 78\n",
      "Processing document 79\n",
      "Processing document 80\n",
      "Processing document 81\n",
      "Processing document 82\n",
      "Processing document 83\n",
      "Processing document 84\n",
      "Processing document 85\n",
      "Processing document 86\n",
      "Processing document 87\n",
      "Processing document 88\n",
      "Processing document 89\n",
      "Processing document 90\n",
      "Processing document 91\n",
      "Processing document 92\n",
      "Processing document 93\n",
      "Processing document 94\n",
      "Processing document 95\n",
      "Processing document 96\n",
      "Processing document 97\n",
      "Processing document 98\n",
      "Processing document 99\n",
      "Processing document 100\n",
      "Processing document 101\n",
      "Truncated document 102 from 361 tokens to 236 tokens\n",
      "Processing document 102\n",
      "Truncated document 103 from 511 tokens to 236 tokens\n",
      "Processing document 103\n",
      "Truncated document 104 from 509 tokens to 236 tokens\n",
      "Processing document 104\n",
      "Truncated document 105 from 396 tokens to 236 tokens\n",
      "Processing document 105\n",
      "Truncated document 106 from 474 tokens to 236 tokens\n",
      "Processing document 106\n",
      "Truncated document 107 from 505 tokens to 236 tokens\n",
      "Processing document 107\n",
      "Processing document 108\n",
      "Processing document 109\n",
      "Processing document 110\n",
      "Processing document 111\n",
      "Processing document 112\n",
      "Processing document 113\n",
      "Processing document 114\n",
      "Processing document 115\n",
      "Processing document 116\n",
      "Processing document 117\n",
      "Processing document 118\n",
      "Processing document 119\n",
      "Processing document 120\n",
      "Processing document 121\n",
      "Truncated document 122 from 465 tokens to 236 tokens\n",
      "Processing document 122\n",
      "Truncated document 123 from 343 tokens to 236 tokens\n",
      "Processing document 123\n",
      "Truncated document 124 from 271 tokens to 236 tokens\n",
      "Processing document 124\n",
      "Truncated document 125 from 286 tokens to 236 tokens\n",
      "Processing document 125\n",
      "Truncated document 126 from 501 tokens to 236 tokens\n",
      "Processing document 126\n",
      "Processing document 127\n",
      "Processing document 128\n",
      "Processing document 129\n",
      "Processing document 130\n",
      "Processing document 131\n",
      "Truncated document 132 from 297 tokens to 236 tokens\n",
      "Processing document 132\n",
      "Processing document 133\n",
      "Processing document 134\n",
      "Processing document 135\n",
      "Processing document 136\n",
      "Processing document 137\n",
      "Processing document 138\n",
      "Processing document 139\n",
      "Processing document 140\n",
      "Processing document 141\n",
      "Truncated document 142 from 514 tokens to 236 tokens\n",
      "Processing document 142\n",
      "Processing document 143\n",
      "Processing document 144\n",
      "Processing document 145\n",
      "Processing document 146\n",
      "Processing document 147\n",
      "Processing document 148\n",
      "Processing document 149\n",
      "Processing document 150\n",
      "Processing document 151\n",
      "Processing document 152\n",
      "Processing document 153\n",
      "Truncated document 154 from 379 tokens to 236 tokens\n",
      "Processing document 154\n",
      "Processing document 155\n",
      "Processing document 156\n",
      "Processing document 157\n",
      "Processing document 158\n",
      "Processing document 159\n",
      "Truncated document 160 from 285 tokens to 236 tokens\n",
      "Processing document 160\n",
      "Processing document 161\n",
      "Processing document 162\n",
      "Processing document 163\n",
      "Processing document 164\n",
      "Processing document 165\n",
      "Truncated document 166 from 276 tokens to 236 tokens\n",
      "Processing document 166\n",
      "Processing document 167\n",
      "Processing document 168\n",
      "Processing document 169\n",
      "Processing document 170\n",
      "Processing document 171\n",
      "Processing document 172\n",
      "Processing document 173\n",
      "Processing document 174\n",
      "Processing document 175\n",
      "Processing document 176\n",
      "Processing document 177\n",
      "Processing document 178\n",
      "Processing document 179\n",
      "Processing document 180\n",
      "Processing document 181\n",
      "Processing document 182\n",
      "Processing document 183\n",
      "Processing document 184\n",
      "Processing document 185\n",
      "Processing document 186\n",
      "Processing document 187\n",
      "Processing document 188\n",
      "Processing document 189\n",
      "Processing document 190\n",
      "Processing document 191\n",
      "Truncated document 192 from 344 tokens to 236 tokens\n",
      "Processing document 192\n",
      "Truncated document 193 from 330 tokens to 236 tokens\n",
      "Processing document 193\n",
      "Processing document 194\n",
      "Truncated document 195 from 317 tokens to 236 tokens\n",
      "Processing document 195\n",
      "Processing document 196\n",
      "Processing document 197\n",
      "Processing document 198\n",
      "Processing document 199\n",
      "Processing document 200\n",
      "Processing document 201\n",
      "Processing document 202\n",
      "Truncated document 203 from 386 tokens to 236 tokens\n",
      "Processing document 203\n",
      "Processing document 204\n",
      "Processing document 205\n",
      "Processing document 206\n",
      "Processing document 207\n",
      "Processing document 208\n",
      "Processing document 209\n",
      "Processing document 210\n",
      "Processing document 211\n",
      "Processing document 212\n",
      "Processing document 213\n",
      "Processing document 214\n",
      "Processing document 215\n",
      "Processing document 216\n",
      "Processing document 217\n",
      "Processing document 218\n",
      "Processing document 219\n",
      "Processing document 220\n",
      "Processing document 221\n",
      "Processing document 222\n",
      "Processing document 223\n",
      "Processing document 224\n",
      "Processing document 225\n",
      "Processing document 226\n",
      "Processing document 227\n",
      "Processing document 228\n",
      "Processing document 229\n",
      "Processing document 230\n",
      "Processing document 231\n",
      "Processing document 232\n",
      "Truncated document 233 from 291 tokens to 236 tokens\n",
      "Processing document 233\n",
      "Processing document 234\n",
      "Processing document 235\n",
      "Processing document 236\n",
      "Processing document 237\n",
      "Truncated document 238 from 275 tokens to 236 tokens\n",
      "Processing document 238\n",
      "Processing document 239\n",
      "Processing document 240\n",
      "Processing document 241\n",
      "Processing document 242\n",
      "Processing document 243\n",
      "Processing document 244\n",
      "Processing document 245\n",
      "Processing document 246\n",
      "Processing document 247\n",
      "Processing document 248\n",
      "Processing document 249\n",
      "Processing document 250\n",
      "Processing document 251\n",
      "Processing document 252\n",
      "Processing document 253\n",
      "Processing document 254\n",
      "Processing document 255\n",
      "Processing document 256\n",
      "Processing document 257\n",
      "Processing document 258\n",
      "Processing document 259\n",
      "Processing document 260\n",
      "Truncated document 261 from 435 tokens to 236 tokens\n",
      "Processing document 261\n",
      "Processing document 262\n",
      "Processing document 263\n",
      "Processing document 264\n",
      "Processing document 265\n",
      "Processing document 266\n",
      "Processing document 267\n",
      "Processing document 268\n",
      "Truncated document 269 from 499 tokens to 236 tokens\n",
      "Processing document 269\n",
      "Processing document 270\n",
      "Processing document 271\n",
      "Processing document 272\n",
      "Processing document 273\n",
      "Processing document 274\n",
      "Processing document 275\n",
      "Truncated document 276 from 496 tokens to 236 tokens\n",
      "Processing document 276\n",
      "Processing document 277\n",
      "Processing document 278\n",
      "Processing document 279\n",
      "Processing document 280\n",
      "Processing document 281\n",
      "Processing document 282\n",
      "Processing document 283\n",
      "Processing document 284\n",
      "Truncated document 285 from 313 tokens to 236 tokens\n",
      "Processing document 285\n",
      "Processing document 286\n",
      "Processing document 287\n",
      "Processing document 288\n",
      "Processing document 289\n",
      "Processing document 290\n",
      "Processing document 291\n",
      "Processing document 292\n",
      "Processing document 293\n",
      "Processing document 294\n",
      "Truncated document 295 from 507 tokens to 236 tokens\n",
      "Processing document 295\n",
      "Processing document 296\n",
      "Processing document 297\n",
      "Processing document 298\n",
      "Processing document 299\n",
      "Processing document 300\n",
      "Processing document 301\n",
      "Processing document 302\n",
      "Processing document 303\n",
      "Processing document 304\n",
      "Processing document 305\n",
      "Processing document 306\n",
      "Processing document 307\n",
      "Truncated document 308 from 528 tokens to 236 tokens\n",
      "Processing document 308\n",
      "Processing document 309\n",
      "Processing document 310\n",
      "Processing document 311\n",
      "Processing document 312\n",
      "Processing document 313\n",
      "Truncated document 314 from 382 tokens to 236 tokens\n",
      "Processing document 314\n",
      "Processing document 315\n",
      "Processing document 316\n",
      "Processing document 317\n",
      "Processing document 318\n",
      "Processing document 319\n",
      "Processing document 320\n",
      "Processing document 321\n",
      "Processing document 322\n",
      "Truncated document 323 from 461 tokens to 236 tokens\n",
      "Processing document 323\n",
      "Processing document 324\n",
      "Processing document 325\n",
      "Processing document 326\n",
      "Processing document 327\n",
      "Processing document 328\n",
      "Processing document 329\n",
      "Processing document 330\n",
      "Processing document 331\n",
      "Processing document 332\n",
      "Processing document 333\n",
      "Processing document 334\n",
      "Processing document 335\n",
      "Processing document 336\n",
      "Processing document 337\n",
      "Processing document 338\n",
      "Processing document 339\n",
      "Truncated document 340 from 422 tokens to 236 tokens\n",
      "Processing document 340\n",
      "Processing document 341\n",
      "Processing document 342\n",
      "Truncated document 343 from 511 tokens to 236 tokens\n",
      "Processing document 343\n",
      "Processing document 344\n",
      "Processing document 345\n",
      "Processing document 346\n",
      "Processing document 347\n",
      "Processing document 348\n",
      "Processing document 349\n",
      "Processing document 350\n",
      "Processing document 351\n",
      "Processing document 352\n",
      "Processing document 353\n",
      "Processing document 354\n",
      "Processing document 355\n",
      "Processing document 356\n",
      "Processing document 357\n",
      "Processing document 358\n",
      "Processing document 359\n",
      "Processing document 360\n",
      "Processing document 361\n",
      "Processing document 362\n",
      "Processing document 363\n",
      "Processing document 364\n",
      "Processing document 365\n",
      "Processing document 366\n",
      "Processing document 367\n",
      "Processing document 368\n",
      "Processing document 369\n",
      "Processing document 370\n",
      "Processing document 371\n",
      "Processing document 372\n",
      "Processing document 373\n",
      "Processing document 374\n",
      "Processing document 375\n",
      "Processing document 376\n",
      "Processing document 377\n",
      "Processing document 378\n",
      "Processing document 379\n",
      "Processing document 380\n",
      "Processing document 381\n",
      "Processing document 382\n",
      "Processing document 383\n",
      "Processing document 384\n",
      "Processing document 385\n",
      "Processing document 386\n",
      "Processing document 387\n",
      "Processing document 388\n",
      "Processing document 389\n",
      "Processing document 390\n",
      "Processing document 391\n",
      "Processing document 392\n",
      "Processing document 393\n",
      "Processing document 394\n",
      "Processing document 395\n",
      "Processing document 396\n",
      "Processing document 397\n",
      "Processing document 398\n",
      "Processing document 399\n",
      "Processing document 400\n",
      "Processing document 401\n",
      "Processing document 402\n",
      "Processing document 403\n",
      "Processing document 404\n",
      "Processing document 405\n",
      "Processing document 406\n",
      "Processing document 407\n",
      "Processing document 408\n",
      "Processing document 409\n",
      "Processing document 410\n",
      "Processing document 411\n",
      "Processing document 412\n",
      "Processing document 413\n",
      "Processing document 414\n",
      "Processing document 415\n",
      "Processing document 416\n",
      "Processing document 417\n",
      "Processing document 418\n",
      "Processing document 419\n",
      "Processing document 420\n",
      "Processing document 421\n",
      "Processing document 422\n",
      "Processing document 423\n",
      "Processing document 424\n",
      "Processing document 425\n",
      "Processing document 426\n",
      "Processing document 427\n",
      "Processing document 428\n",
      "Processing document 429\n",
      "Processing document 430\n",
      "Processing document 431\n",
      "Processing document 432\n",
      "Processing document 433\n",
      "Processing document 434\n",
      "Processing document 435\n",
      "Processing document 436\n",
      "Processing document 437\n",
      "Processing document 438\n",
      "Processing document 439\n",
      "Processing document 440\n",
      "Processing document 441\n",
      "Processing document 442\n",
      "Processing document 443\n",
      "Processing document 444\n",
      "Processing document 445\n",
      "Processing document 446\n",
      "Processing document 447\n",
      "Processing document 448\n",
      "Processing document 449\n",
      "Processing document 450\n",
      "Processing document 451\n",
      "Processing document 452\n",
      "Processing document 453\n",
      "Processing document 454\n",
      "Processing document 455\n",
      "Processing document 456\n",
      "Processing document 457\n",
      "Processing document 458\n",
      "Processing document 459\n",
      "Processing document 460\n",
      "Processing document 461\n",
      "Processing document 462\n",
      "Processing document 463\n",
      "Processing document 464\n",
      "Processing document 465\n",
      "Processing document 466\n",
      "Processing document 467\n",
      "Processing document 468\n",
      "Processing document 469\n",
      "Processing document 470\n",
      "Processing document 471\n",
      "Processing document 472\n",
      "Processing document 473\n",
      "Processing document 474\n",
      "Processing document 475\n",
      "Processing document 476\n",
      "Processing document 477\n",
      "Processing document 478\n",
      "Processing document 479\n",
      "Processing document 480\n",
      "Processing document 481\n",
      "Processing document 482\n",
      "Processing document 483\n",
      "Processing document 484\n",
      "Processing document 485\n",
      "Processing document 486\n",
      "Processing document 487\n",
      "Processing document 488\n",
      "Processing document 489\n",
      "Processing document 490\n",
      "Processing document 491\n",
      "Processing document 492\n",
      "Processing document 493\n",
      "Processing document 494\n",
      "Processing document 495\n",
      "Processing document 496\n",
      "Processing document 497\n",
      "Processing document 498\n",
      "Processing document 499\n",
      "Processing document 500\n",
      "Processing document 501\n",
      "Processing document 502\n",
      "Processing document 503\n",
      "Truncated document 504 from 433 tokens to 236 tokens\n",
      "Processing document 504\n",
      "Truncated document 505 from 421 tokens to 236 tokens\n",
      "Processing document 505\n",
      "Truncated document 506 from 285 tokens to 236 tokens\n",
      "Processing document 506\n",
      "Truncated document 507 from 282 tokens to 236 tokens\n",
      "Processing document 507\n",
      "Truncated document 508 from 474 tokens to 236 tokens\n",
      "Processing document 508\n",
      "Truncated document 509 from 353 tokens to 236 tokens\n",
      "Processing document 509\n",
      "Processing document 510\n",
      "Truncated document 511 from 288 tokens to 236 tokens\n",
      "Processing document 511\n",
      "Processing document 512\n",
      "Processing document 513\n",
      "Processing document 514\n",
      "Truncated document 515 from 310 tokens to 236 tokens\n",
      "Processing document 515\n",
      "Processing document 516\n",
      "Processing document 517\n",
      "Truncated document 518 from 287 tokens to 236 tokens\n",
      "Processing document 518\n",
      "Processing document 519\n",
      "Processing document 520\n",
      "Processing document 521\n",
      "Processing document 522\n",
      "Processing document 523\n",
      "Truncated document 524 from 257 tokens to 236 tokens\n",
      "Processing document 524\n",
      "Processing document 525\n",
      "Processing document 526\n",
      "Processing document 527\n",
      "Truncated document 528 from 317 tokens to 236 tokens\n",
      "Processing document 528\n",
      "Truncated document 529 from 326 tokens to 236 tokens\n",
      "Processing document 529\n",
      "Processing document 530\n",
      "Processing document 531\n",
      "Processing document 532\n",
      "Truncated document 533 from 317 tokens to 236 tokens\n",
      "Processing document 533\n",
      "Processing document 534\n",
      "Processing document 535\n",
      "Truncated document 536 from 405 tokens to 236 tokens\n",
      "Processing document 536\n",
      "Truncated document 537 from 426 tokens to 236 tokens\n",
      "Processing document 537\n",
      "Truncated document 538 from 512 tokens to 236 tokens\n",
      "Processing document 538\n",
      "Truncated document 539 from 291 tokens to 236 tokens\n",
      "Processing document 539\n",
      "Truncated document 540 from 458 tokens to 236 tokens\n",
      "Processing document 540\n",
      "Truncated document 541 from 522 tokens to 236 tokens\n",
      "Processing document 541\n",
      "Truncated document 542 from 496 tokens to 236 tokens\n",
      "Processing document 542\n",
      "Truncated document 543 from 446 tokens to 236 tokens\n",
      "Processing document 543\n",
      "Processing document 544\n",
      "Processing document 545\n",
      "Processing document 546\n",
      "Processing document 547\n",
      "Processing document 548\n",
      "Processing document 549\n",
      "Processing document 550\n",
      "Processing document 551\n",
      "Processing document 552\n",
      "Processing document 553\n",
      "Processing document 554\n",
      "Processing document 555\n",
      "Processing document 556\n",
      "Processing document 557\n",
      "Processing document 558\n",
      "Processing document 559\n",
      "Processing document 560\n",
      "Processing document 561\n",
      "Processing document 562\n",
      "Processing document 563\n",
      "Processing document 564\n",
      "Processing document 565\n",
      "Processing document 566\n",
      "Processing document 567\n",
      "Processing document 568\n",
      "Processing document 569\n",
      "Processing document 570\n",
      "Processing document 571\n",
      "Processing document 572\n",
      "Processing document 573\n",
      "Processing document 574\n",
      "Processing document 575\n",
      "Processing document 576\n",
      "Processing document 577\n",
      "Processing document 578\n",
      "Processing document 579\n",
      "Processing document 580\n",
      "Processing document 581\n",
      "Truncated document 582 from 366 tokens to 236 tokens\n",
      "Processing document 582\n",
      "Truncated document 583 from 440 tokens to 236 tokens\n",
      "Processing document 583\n",
      "Truncated document 584 from 401 tokens to 236 tokens\n",
      "Processing document 584\n",
      "Truncated document 585 from 344 tokens to 236 tokens\n",
      "Processing document 585\n",
      "Truncated document 586 from 440 tokens to 236 tokens\n",
      "Processing document 586\n",
      "Processing document 587\n",
      "Processing document 588\n",
      "Processing document 589\n",
      "Processing document 590\n",
      "Processing document 591\n",
      "Processing document 592\n",
      "Processing document 593\n",
      "Processing document 594\n",
      "Processing document 595\n",
      "Processing document 596\n",
      "Processing document 597\n",
      "Processing document 598\n",
      "Processing document 599\n",
      "Processing document 600\n",
      "Processing document 601\n",
      "Processing document 602\n",
      "Processing document 603\n",
      "Processing document 604\n",
      "Processing document 605\n",
      "Processing document 606\n",
      "Processing document 607\n",
      "Processing document 608\n",
      "Processing document 609\n",
      "Processing document 610\n",
      "Processing document 611\n",
      "Processing document 612\n",
      "Processing document 613\n",
      "Processing document 614\n",
      "Processing document 615\n",
      "Processing document 616\n",
      "Processing document 617\n",
      "Processing document 618\n",
      "Processing document 619\n",
      "Processing document 620\n",
      "Processing document 621\n",
      "Processing document 622\n",
      "Processing document 623\n",
      "Processing document 624\n",
      "Processing document 625\n",
      "Processing document 626\n",
      "Processing document 627\n",
      "Processing document 628\n",
      "Processing document 629\n",
      "Processing document 630\n",
      "Truncated document 631 from 400 tokens to 236 tokens\n",
      "Processing document 631\n",
      "Truncated document 632 from 409 tokens to 236 tokens\n",
      "Processing document 632\n",
      "Processing document 633\n",
      "Processing document 634\n",
      "Truncated document 635 from 462 tokens to 236 tokens\n",
      "Processing document 635\n",
      "Processing document 636\n",
      "Truncated document 637 from 347 tokens to 236 tokens\n",
      "Processing document 637\n",
      "Truncated document 638 from 270 tokens to 236 tokens\n",
      "Processing document 638\n",
      "Processing document 639\n",
      "Processing document 640\n",
      "Processing document 641\n",
      "Processing document 642\n",
      "Processing document 643\n",
      "Processing document 644\n",
      "Processing document 645\n",
      "Processing document 646\n",
      "Processing document 647\n",
      "Processing document 648\n",
      "Processing document 649\n",
      "Processing document 650\n",
      "Processing document 651\n",
      "Processing document 652\n",
      "Processing document 653\n",
      "Processing document 654\n",
      "Processing document 655\n",
      "Processing document 656\n",
      "Processing document 657\n",
      "Processing document 658\n",
      "Processing document 659\n",
      "Processing document 660\n",
      "Processing document 661\n",
      "Processing document 662\n",
      "Processing document 663\n",
      "Processing document 664\n",
      "Truncated document 665 from 314 tokens to 236 tokens\n",
      "Processing document 665\n",
      "Processing document 666\n",
      "Processing document 667\n",
      "Processing document 668\n",
      "Truncated document 669 from 458 tokens to 236 tokens\n",
      "Processing document 669\n",
      "Processing document 670\n",
      "Processing document 671\n",
      "Processing document 672\n",
      "Processing document 673\n",
      "Processing document 674\n",
      "Processing document 675\n",
      "Processing document 676\n",
      "Processing document 677\n",
      "Processing document 678\n",
      "Processing document 679\n",
      "Processing document 680\n",
      "Processing document 681\n",
      "Processing document 682\n",
      "Processing document 683\n",
      "Processing document 684\n",
      "Processing document 685\n",
      "Processing document 686\n",
      "Processing document 687\n",
      "Processing document 688\n",
      "Processing document 689\n",
      "Processing document 690\n",
      "Processing document 691\n",
      "Processing document 692\n",
      "Processing document 693\n",
      "Processing document 694\n",
      "Processing document 695\n",
      "Processing document 696\n",
      "Processing document 697\n",
      "Processing document 698\n",
      "Processing document 699\n",
      "Processing document 700\n",
      "Processing document 701\n",
      "Truncated document 702 from 437 tokens to 236 tokens\n",
      "Processing document 702\n",
      "Processing document 703\n",
      "Truncated document 704 from 539 tokens to 236 tokens\n",
      "Processing document 704\n",
      "Truncated document 705 from 449 tokens to 236 tokens\n",
      "Processing document 705\n",
      "Truncated document 706 from 517 tokens to 236 tokens\n",
      "Processing document 706\n",
      "Processing document 707\n",
      "Processing document 708\n",
      "Processing document 709\n",
      "Processing document 710\n",
      "Processing document 711\n",
      "Processing document 712\n",
      "Processing document 713\n",
      "Processing document 714\n",
      "Processing document 715\n",
      "Processing document 716\n",
      "Processing document 717\n",
      "Processing document 718\n",
      "Processing document 719\n",
      "Processing document 720\n",
      "Processing document 721\n",
      "Processing document 722\n",
      "Processing document 723\n",
      "Processing document 724\n",
      "Processing document 725\n",
      "Processing document 726\n",
      "Processing document 727\n",
      "Truncated document 728 from 258 tokens to 236 tokens\n",
      "Processing document 728\n",
      "Truncated document 729 from 363 tokens to 236 tokens\n",
      "Processing document 729\n",
      "Truncated document 730 from 480 tokens to 236 tokens\n",
      "Processing document 730\n",
      "Truncated document 731 from 496 tokens to 236 tokens\n",
      "Processing document 731\n",
      "Truncated document 732 from 277 tokens to 236 tokens\n",
      "Processing document 732\n",
      "Truncated document 733 from 514 tokens to 236 tokens\n",
      "Processing document 733\n",
      "Processing document 734\n",
      "Processing document 735\n",
      "Processing document 736\n",
      "Processing document 737\n",
      "Processing document 738\n",
      "Processing document 739\n",
      "Processing document 740\n",
      "Processing document 741\n",
      "Processing document 742\n",
      "Processing document 743\n",
      "Processing document 744\n",
      "Processing document 745\n",
      "Processing document 746\n",
      "Processing document 747\n",
      "Processing document 748\n",
      "Processing document 749\n",
      "Processing document 750\n",
      "Processing document 751\n",
      "Processing document 752\n",
      "Processing document 753\n",
      "Processing document 754\n",
      "Processing document 755\n",
      "Processing document 756\n",
      "Processing document 757\n",
      "Processing document 758\n",
      "Processing document 759\n",
      "Processing document 760\n",
      "Processing document 761\n",
      "Truncated document 762 from 396 tokens to 236 tokens\n",
      "Processing document 762\n",
      "Truncated document 763 from 486 tokens to 236 tokens\n",
      "Processing document 763\n",
      "Truncated document 764 from 362 tokens to 236 tokens\n",
      "Processing document 764\n",
      "Truncated document 765 from 442 tokens to 236 tokens\n",
      "Processing document 765\n",
      "Truncated document 766 from 497 tokens to 236 tokens\n",
      "Processing document 766\n",
      "Processing document 767\n",
      "Processing document 768\n",
      "Processing document 769\n",
      "Processing document 770\n",
      "Processing document 771\n",
      "Processing document 772\n",
      "Processing document 773\n",
      "Processing document 774\n",
      "Processing document 775\n",
      "Processing document 776\n",
      "Processing document 777\n",
      "Processing document 778\n",
      "Processing document 779\n",
      "Processing document 780\n",
      "Processing document 781\n",
      "Processing document 782\n",
      "Processing document 783\n",
      "Processing document 784\n",
      "Processing document 785\n",
      "Processing document 786\n",
      "Processing document 787\n",
      "Processing document 788\n",
      "Processing document 789\n",
      "Processing document 790\n",
      "Processing document 791\n",
      "Processing document 792\n",
      "Truncated document 793 from 265 tokens to 236 tokens\n",
      "Processing document 793\n",
      "Processing document 794\n",
      "Processing document 795\n",
      "Truncated document 796 from 297 tokens to 236 tokens\n",
      "Processing document 796\n",
      "Truncated document 797 from 326 tokens to 236 tokens\n",
      "Processing document 797\n",
      "Processing document 798\n",
      "Processing document 799\n",
      "Processing document 800\n",
      "Processing document 801\n",
      "Processing document 802\n",
      "Processing document 803\n",
      "Processing document 804\n",
      "Processing document 805\n",
      "Processing document 806\n",
      "Processing document 807\n",
      "Processing document 808\n",
      "Processing document 809\n",
      "Processing document 810\n",
      "Processing document 811\n",
      "Processing document 812\n",
      "Processing document 813\n",
      "Processing document 814\n",
      "Processing document 815\n",
      "Processing document 816\n",
      "Processing document 817\n",
      "Processing document 818\n",
      "Processing document 819\n",
      "Processing document 820\n",
      "Processing document 821\n",
      "Processing document 822\n",
      "Processing document 823\n",
      "Processing document 824\n",
      "Processing document 825\n",
      "Processing document 826\n",
      "Processing document 827\n",
      "Processing document 828\n",
      "Processing document 829\n",
      "Processing document 830\n",
      "Processing document 831\n",
      "Processing document 832\n",
      "Processing document 833\n",
      "Processing document 834\n",
      "Processing document 835\n",
      "Processing document 836\n",
      "Processing document 837\n",
      "Truncated document 838 from 309 tokens to 236 tokens\n",
      "Processing document 838\n",
      "Truncated document 839 from 444 tokens to 236 tokens\n",
      "Processing document 839\n",
      "Truncated document 840 from 321 tokens to 236 tokens\n",
      "Processing document 840\n",
      "Truncated document 841 from 401 tokens to 236 tokens\n",
      "Processing document 841\n",
      "Truncated document 842 from 460 tokens to 236 tokens\n",
      "Processing document 842\n",
      "Truncated document 843 from 544 tokens to 236 tokens\n",
      "Processing document 843\n",
      "Truncated document 844 from 523 tokens to 236 tokens\n",
      "Processing document 844\n",
      "Processing document 845\n",
      "Processing document 846\n",
      "Processing document 847\n",
      "Processing document 848\n",
      "Processing document 849\n",
      "Processing document 850\n",
      "Processing document 851\n",
      "Processing document 852\n",
      "Processing document 853\n",
      "Processing document 854\n",
      "Processing document 855\n",
      "Processing document 856\n",
      "Processing document 857\n",
      "Processing document 858\n",
      "Processing document 859\n",
      "Processing document 860\n",
      "Processing document 861\n",
      "Processing document 862\n",
      "Processing document 863\n",
      "Processing document 864\n",
      "Processing document 865\n",
      "Processing document 866\n",
      "Processing document 867\n",
      "Processing document 868\n",
      "Processing document 869\n",
      "Processing document 870\n",
      "Processing document 871\n",
      "Processing document 872\n",
      "Processing document 873\n",
      "Processing document 874\n",
      "Truncated document 875 from 269 tokens to 236 tokens\n",
      "Processing document 875\n",
      "Truncated document 876 from 463 tokens to 236 tokens\n",
      "Processing document 876\n",
      "Truncated document 877 from 264 tokens to 236 tokens\n",
      "Processing document 877\n",
      "Truncated document 878 from 506 tokens to 236 tokens\n",
      "Processing document 878\n",
      "Truncated document 879 from 338 tokens to 236 tokens\n",
      "Processing document 879\n",
      "Truncated document 880 from 482 tokens to 236 tokens\n",
      "Processing document 880\n",
      "Processing document 881\n",
      "Processing document 882\n",
      "Processing document 883\n",
      "Processing document 884\n",
      "Processing document 885\n",
      "Processing document 886\n",
      "Processing document 887\n",
      "Processing document 888\n",
      "Processing document 889\n",
      "Processing document 890\n",
      "Processing document 891\n",
      "Processing document 892\n",
      "Processing document 893\n",
      "Processing document 894\n",
      "Processing document 895\n",
      "Processing document 896\n",
      "Processing document 897\n",
      "Processing document 898\n",
      "Processing document 899\n",
      "Processing document 900\n",
      "Processing document 901\n",
      "Processing document 902\n",
      "Processing document 903\n",
      "Processing document 904\n",
      "Processing document 905\n",
      "Processing document 906\n",
      "Processing document 907\n",
      "Processing document 908\n",
      "Processing document 909\n",
      "Processing document 910\n",
      "Processing document 911\n",
      "Processing document 912\n",
      "Processing document 913\n",
      "Processing document 914\n",
      "Processing document 915\n",
      "Processing document 916\n",
      "Processing document 917\n",
      "Processing document 918\n",
      "Processing document 919\n",
      "Processing document 920\n",
      "Processing document 921\n",
      "Processing document 922\n",
      "Processing document 923\n",
      "Processing document 924\n",
      "Processing document 925\n",
      "Processing document 926\n",
      "Processing document 927\n",
      "Processing document 928\n",
      "Processing document 929\n",
      "Processing document 930\n",
      "Processing document 931\n",
      "Processing document 932\n",
      "Processing document 933\n",
      "Processing document 934\n",
      "Processing document 935\n",
      "Processing document 936\n",
      "Processing document 937\n",
      "Processing document 938\n",
      "Processing document 939\n",
      "Truncated document 940 from 420 tokens to 236 tokens\n",
      "Processing document 940\n",
      "Truncated document 941 from 292 tokens to 236 tokens\n",
      "Processing document 941\n",
      "Processing document 942\n",
      "Truncated document 943 from 392 tokens to 236 tokens\n",
      "Processing document 943\n",
      "Processing document 944\n",
      "Processing document 945\n",
      "Processing document 946\n",
      "Processing document 947\n",
      "Truncated document 948 from 279 tokens to 236 tokens\n",
      "Processing document 948\n",
      "Processing document 949\n",
      "Processing document 950\n",
      "Processing document 951\n",
      "Processing document 952\n",
      "Truncated document 953 from 351 tokens to 236 tokens\n",
      "Processing document 953\n",
      "Processing document 954\n",
      "Processing document 955\n",
      "Processing document 956\n",
      "Processing document 957\n",
      "Processing document 958\n",
      "Processing document 959\n",
      "Processing document 960\n",
      "Processing document 961\n",
      "Processing document 962\n",
      "Processing document 963\n",
      "Processing document 964\n",
      "Processing document 965\n",
      "Processing document 966\n",
      "Processing document 967\n",
      "Processing document 968\n",
      "Processing document 969\n",
      "Processing document 970\n",
      "Processing document 971\n",
      "Processing document 972\n",
      "Processing document 973\n",
      "Processing document 974\n",
      "Processing document 975\n",
      "Processing document 976\n",
      "Processing document 977\n",
      "Processing document 978\n",
      "Processing document 979\n",
      "Processing document 980\n",
      "Processing document 981\n",
      "Processing document 982\n",
      "Truncated document 983 from 402 tokens to 236 tokens\n",
      "Processing document 983\n",
      "Processing document 984\n",
      "Processing document 985\n",
      "Processing document 986\n",
      "Processing document 987\n",
      "Processing document 988\n",
      "Processing document 989\n",
      "Truncated document 990 from 497 tokens to 236 tokens\n",
      "Processing document 990\n",
      "Processing document 991\n",
      "Processing document 992\n",
      "Processing document 993\n",
      "Processing document 994\n",
      "Processing document 995\n",
      "Processing document 996\n",
      "Processing document 997\n",
      "Processing document 998\n",
      "Processing document 999\n",
      "Processing document 1000\n",
      "Processing document 1001\n",
      "Truncated document 1002 from 514 tokens to 236 tokens\n",
      "Processing document 1002\n",
      "Processing document 1003\n",
      "Processing document 1004\n",
      "Processing document 1005\n",
      "Processing document 1006\n",
      "Processing document 1007\n",
      "Processing document 1008\n",
      "Processing document 1009\n",
      "Processing document 1010\n",
      "Processing document 1011\n",
      "Truncated document 1012 from 493 tokens to 236 tokens\n",
      "Processing document 1012\n",
      "Processing document 1013\n",
      "Processing document 1014\n",
      "Processing document 1015\n",
      "Processing document 1016\n",
      "Processing document 1017\n",
      "Processing document 1018\n",
      "Processing document 1019\n",
      "Processing document 1020\n",
      "Processing document 1021\n",
      "Processing document 1022\n",
      "Processing document 1023\n",
      "Processing document 1024\n",
      "Processing document 1025\n",
      "Processing document 1026\n",
      "Truncated document 1027 from 303 tokens to 236 tokens\n",
      "Processing document 1027\n",
      "Truncated document 1028 from 338 tokens to 236 tokens\n",
      "Processing document 1028\n",
      "Truncated document 1029 from 511 tokens to 236 tokens\n",
      "Processing document 1029\n",
      "Truncated document 1030 from 501 tokens to 236 tokens\n",
      "Processing document 1030\n",
      "Processing document 1031\n",
      "Processing document 1032\n",
      "Processing document 1033\n",
      "Processing document 1034\n",
      "Processing document 1035\n",
      "Processing document 1036\n",
      "Processing document 1037\n",
      "Processing document 1038\n",
      "Processing document 1039\n",
      "Truncated document 1040 from 534 tokens to 236 tokens\n",
      "Processing document 1040\n",
      "Processing document 1041\n",
      "Processing document 1042\n",
      "Processing document 1043\n",
      "Processing document 1044\n",
      "Processing document 1045\n",
      "Processing document 1046\n",
      "Processing document 1047\n",
      "Processing document 1048\n",
      "Processing document 1049\n",
      "Processing document 1050\n",
      "Processing document 1051\n",
      "Processing document 1052\n",
      "Processing document 1053\n",
      "Processing document 1054\n",
      "Truncated document 1055 from 477 tokens to 236 tokens\n",
      "Processing document 1055\n",
      "Truncated document 1056 from 267 tokens to 236 tokens\n",
      "Processing document 1056\n",
      "Processing document 1057\n",
      "Truncated document 1058 from 489 tokens to 236 tokens\n",
      "Processing document 1058\n",
      "Processing document 1059\n",
      "Processing document 1060\n",
      "Processing document 1061\n",
      "Processing document 1062\n",
      "Processing document 1063\n",
      "Processing document 1064\n",
      "Truncated document 1065 from 416 tokens to 236 tokens\n",
      "Processing document 1065\n",
      "Processing document 1066\n",
      "Processing document 1067\n",
      "Processing document 1068\n",
      "Processing document 1069\n",
      "Processing document 1070\n",
      "Processing document 1071\n",
      "Processing document 1072\n",
      "Processing document 1073\n",
      "Processing document 1074\n",
      "Processing document 1075\n",
      "Processing document 1076\n",
      "Processing document 1077\n",
      "Processing document 1078\n",
      "Processing document 1079\n",
      "Processing document 1080\n",
      "Processing document 1081\n",
      "Processing document 1082\n",
      "Processing document 1083\n",
      "Processing document 1084\n",
      "Processing document 1085\n",
      "Processing document 1086\n",
      "Processing document 1087\n",
      "Processing document 1088\n",
      "Processing document 1089\n",
      "Processing document 1090\n",
      "Processing document 1091\n",
      "Processing document 1092\n",
      "Processing document 1093\n",
      "Processing document 1094\n",
      "Processing document 1095\n",
      "Processing document 1096\n",
      "Processing document 1097\n",
      "Processing document 1098\n",
      "Processing document 1099\n",
      "Processing document 1100\n",
      "Processing document 1101\n",
      "Processing document 1102\n",
      "Processing document 1103\n",
      "Processing document 1104\n",
      "Processing document 1105\n",
      "Processing document 1106\n",
      "Processing document 1107\n",
      "Processing document 1108\n",
      "Processing document 1109\n",
      "Truncated document 1110 from 382 tokens to 236 tokens\n",
      "Processing document 1110\n",
      "Truncated document 1111 from 434 tokens to 236 tokens\n",
      "Processing document 1111\n",
      "Truncated document 1112 from 304 tokens to 236 tokens\n",
      "Processing document 1112\n",
      "Truncated document 1113 from 372 tokens to 236 tokens\n",
      "Processing document 1113\n",
      "Truncated document 1114 from 476 tokens to 236 tokens\n",
      "Processing document 1114\n",
      "Processing document 1115\n",
      "Truncated document 1116 from 382 tokens to 236 tokens\n",
      "Processing document 1116\n",
      "Truncated document 1117 from 473 tokens to 236 tokens\n",
      "Processing document 1117\n",
      "Processing document 1118\n",
      "Processing document 1119\n",
      "Processing document 1120\n",
      "Processing document 1121\n",
      "Processing document 1122\n",
      "Processing document 1123\n",
      "Processing document 1124\n",
      "Processing document 1125\n",
      "Processing document 1126\n",
      "Processing document 1127\n",
      "Processing document 1128\n",
      "Truncated document 1129 from 482 tokens to 236 tokens\n",
      "Processing document 1129\n",
      "Processing document 1130\n",
      "Processing document 1131\n",
      "Processing document 1132\n",
      "Processing document 1133\n",
      "Processing document 1134\n",
      "Processing document 1135\n",
      "Processing document 1136\n",
      "Processing document 1137\n",
      "Processing document 1138\n",
      "Processing document 1139\n",
      "Processing document 1140\n",
      "Truncated document 1141 from 508 tokens to 236 tokens\n",
      "Processing document 1141\n",
      "Processing document 1142\n",
      "Processing document 1143\n",
      "Processing document 1144\n",
      "Processing document 1145\n",
      "Processing document 1146\n",
      "Processing document 1147\n",
      "Processing document 1148\n",
      "Processing document 1149\n",
      "Processing document 1150\n",
      "Processing document 1151\n",
      "Processing document 1152\n",
      "Processing document 1153\n",
      "Processing document 1154\n",
      "Processing document 1155\n",
      "Processing document 1156\n",
      "Processing document 1157\n",
      "Processing document 1158\n",
      "Processing document 1159\n",
      "Truncated document 1160 from 510 tokens to 236 tokens\n",
      "Processing document 1160\n",
      "Truncated document 1161 from 533 tokens to 236 tokens\n",
      "Processing document 1161\n",
      "Truncated document 1162 from 294 tokens to 236 tokens\n",
      "Processing document 1162\n",
      "Processing document 1163\n",
      "Processing document 1164\n",
      "Processing document 1165\n",
      "Processing document 1166\n",
      "Processing document 1167\n",
      "Processing document 1168\n",
      "Processing document 1169\n",
      "Processing document 1170\n",
      "Processing document 1171\n",
      "Processing document 1172\n",
      "Processing document 1173\n",
      "Processing document 1174\n",
      "Processing document 1175\n",
      "Processing document 1176\n",
      "Processing document 1177\n",
      "Processing document 1178\n",
      "Processing document 1179\n",
      "Processing document 1180\n",
      "Processing document 1181\n",
      "Processing document 1182\n",
      "Processing document 1183\n",
      "Processing document 1184\n",
      "Processing document 1185\n",
      "Processing document 1186\n",
      "Processing document 1187\n",
      "Processing document 1188\n",
      "Truncated document 1189 from 258 tokens to 236 tokens\n",
      "Processing document 1189\n",
      "Processing document 1190\n",
      "Processing document 1191\n",
      "Processing document 1192\n",
      "Processing document 1193\n",
      "Processing document 1194\n",
      "Truncated document 1195 from 390 tokens to 236 tokens\n",
      "Processing document 1195\n",
      "Processing document 1196\n",
      "Processing document 1197\n",
      "Total documents truncated: 135\n",
      "Indexing completed. Time taken: 170.58 seconds\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "from time import time\n",
    "es = Elasticsearch(  #new 05/02/25\n",
    "    \"https://my-elasticsearch-project-acc1b0.es.ap-southeast-1.aws.elastic.cloud:443\",\n",
    "    api_key=\"SFJsUTFKUUJrYVpaM1NIdzdxNlo6ei1NVlA3azZTTy1RbllvMzBEY0kxUQ==\"\n",
    ")\n",
    "\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     \"https://my-elasticsearch-project-fc9fd1.es.ap-southeast-1.aws.elastic.cloud:443\",\n",
    "#     api_key=\"aXY4NkpKUUJaNVNfUEdnZHdVZ186MExmVk1iclZRWWlrS1hpeDRhOWRGUQ==\"\n",
    "# )\n",
    "\n",
    "from model_config import load_embedding_model_VN2, load_tokenizer2\n",
    "embeddings = load_embedding_model_VN2()\n",
    "tokenizer = load_tokenizer2()\n",
    "json_file_path = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\finaldf.json\"\n",
    "index_name = \"raptor\"\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"metadata\": {\"type\": \"object\"},\n",
    "            \"vector\": {\"type\": \"dense_vector\", \"dims\": 768, \"similarity\": \"cosine\", \"index\": True}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "def generate_actions(documents):\n",
    "    truncated = 0\n",
    "    for i, record in enumerate(documents):\n",
    "        try:\n",
    "            # Get tokens\n",
    "            tokens = tokenizer.encode(record[\"text\"], add_special_tokens=False)\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if len(tokens) > 254:\n",
    "                token = tokens[:236]  # Chỉ lấy 254 token đầu tiên\n",
    "                record[\"text\"] = tokenizer.decode(token)  # Chuyển lại thành text\n",
    "                truncated += 1\n",
    "                print(f\"Truncated document {i} from {len(tokens)} tokens to 236 tokens\")\n",
    "                \n",
    "            embedding = embeddings.embed_query(record[\"text\"])\n",
    "            print(f\"Processing document {i}\")\n",
    "            yield {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": {\n",
    "                    \"text\": record[\"text\"],\n",
    "                    \"metadata\": record[\"metadata\"],\n",
    "                    \"vector\": embedding\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {i}: {str(e)}\")\n",
    "            raise e\n",
    "    print(f\"Total documents truncated: {truncated}\")\n",
    "\n",
    "\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "start_time = time()\n",
    "helpers.bulk(es, generate_actions(json_data))\n",
    "print(f\"Indexing completed. Time taken: {time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
