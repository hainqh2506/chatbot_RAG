{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "from model_config import load_embedding_model_VN2\n",
    "\n",
    "def upload_chunks_to_elasticsearch(json_file_path: str, index_name: str):\n",
    "    \"\"\"\n",
    "    Đọc file JSON, tạo embedding và đưa dữ liệu lên Elasticsearch.\n",
    "    \"\"\"\n",
    "    embeddings = load_embedding_model_VN2()\n",
    "\n",
    "    # Khởi tạo Elasticsearch client\n",
    "    es = Elasticsearch(\n",
    "        hosts=[\"http://localhost:9200\"],\n",
    "        basic_auth=(\"elastic\", \"BKELASTIC\"),\n",
    "        verify_certs=False\n",
    "    )\n",
    "    json_file_path = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\data.json\"\n",
    "    index_name = \"my_index\" \n",
    "    # Kiểm tra nếu index đã tồn tại, xóa và tạo mới\n",
    "    if es.indices.exists(index=index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "    es.indices.create(index=index_name, body=mapping)  # Sử dụng mapping bạn đã định nghĩa\n",
    "\n",
    "    # Đọc file JSON\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Chuyển đổi json_data thành định dạng Elasticsearch bulk\n",
    "    def generate_actions(documents):\n",
    "        for record in documents:\n",
    "            # Tạo vector embedding từ nội dung text\n",
    "            embedding = embeddings.embed_query(record[\"text\"])\n",
    "\n",
    "            yield {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": {\n",
    "                    \"text\": record[\"text\"],\n",
    "                    \"metadata\": record[\"metadata\"],\n",
    "                    \"vector\": embedding\n",
    "                }\n",
    "            }\n",
    "\n",
    "    # Sử dụng helpers.bulk để insert batch dữ liệu vào Elasticsearch\n",
    "    helpers.bulk(es, generate_actions(json_data))\n",
    "\n",
    "    print(f\"Đã chèn {len(json_data)} bản ghi từ file JSON vào Elasticsearch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\data.json\"\n",
    "index_name = \"my_index\"  # Thay đổi index name nếu cần\n",
    "upload_to_elasticsearch(output_json_file, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test do dai doan van ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding = load_embedding_model_VN2()\n",
    "tokenizer = load_tokenizer2()\n",
    "\n",
    "\n",
    "\n",
    "def test_text_length(text: str, tokenizer, model_embedding):\n",
    "    \"\"\"\n",
    "    Thử nghiệm với độ dài text tăng dần và ghi lại độ dài tối đa có thể tạo embedding.\n",
    "\n",
    "    Args:\n",
    "        text: Đoạn text cần thử nghiệm.\n",
    "        tokenizer: Tokenizer của model embedding.\n",
    "        model_embedding: Model embedding.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    max_chars = 0\n",
    "    max_tokens = 0\n",
    "    error_occurred = False\n",
    "\n",
    "    for i in range(1, len(words) + 1):\n",
    "        partial_text = \" \".join(words[:i])\n",
    "        num_chars = len(partial_text)\n",
    "        num_tokens = len(tokenizer.encode(partial_text, add_special_tokens=True))\n",
    "\n",
    "        try:\n",
    "            model_embedding.embed_query(partial_text)\n",
    "            max_chars = num_chars\n",
    "            max_tokens = num_tokens\n",
    "            print(f\"Processed successfully up to {i} words, {num_chars} chars, {num_tokens} tokens.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {i} words ({num_chars} chars, {num_tokens} tokens): {e}\")\n",
    "            print(f\"Problematic text: {partial_text}\")\n",
    "            error_occurred = True\n",
    "            break\n",
    "\n",
    "    if not error_occurred:\n",
    "        print(f\"Successfully processed entire text: {len(text)} chars, {len(tokenizer.encode(text, add_special_tokens=True))} tokens\")\n",
    "    else:\n",
    "        print(f\"Max processed characters: {max_chars}\")\n",
    "        print(f\"Max processed tokens: {max_tokens}\")\n",
    "\n",
    "    return error_occurred, max_chars, max_tokens\n",
    "\n",
    "# Thử nghiệm với document 75\n",
    "text_75 = json_data[75][\"text\"]\n",
    "print(\"Testing document 75:\")\n",
    "test_text_length(text_75, tokenizer, model_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test xem co doan nao >255 token hay k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_config import load_embedding_model_VN2 ,load_tokenizer2\n",
    "import json\n",
    "\n",
    "def check_text_length(json_file_path: str, max_length: int = 255):\n",
    "    \"\"\"\n",
    "    Kiểm tra độ dài token của các text trong file JSON và in ra các text vượt quá giới hạn.\n",
    "\n",
    "    Args:\n",
    "        json_file_path: Đường dẫn đến file JSON.\n",
    "        max_length: Độ dài token tối đa.\n",
    "    \"\"\"\n",
    "    embeddings = load_embedding_model_VN2()\n",
    "    tokenizer = load_tokenizer2() # Sử dụng tokenizer của model embedding\n",
    "\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    problematic_records = []\n",
    "\n",
    "    for i, record in enumerate(json_data):\n",
    "        text = record[\"text\"]\n",
    "        tokens = tokenizer.encode(text)\n",
    "        length = len(tokens)\n",
    "\n",
    "        if length > max_length:\n",
    "            problematic_records.append({\n",
    "                \"index\": i,\n",
    "                \"length\": length,\n",
    "                \"text\": text[:200] + \"...\"  # Chỉ in ra 200 ký tự đầu tiên\n",
    "            })\n",
    "\n",
    "    if problematic_records:\n",
    "        print(f\"Tìm thấy {len(problematic_records)} bản ghi có độ dài vượt quá {max_length} tokens:\")\n",
    "        for rec in problematic_records:\n",
    "            print(f\"- Index: {rec['index']}, Length: {rec['length']}, Text: {rec['text']}\")\n",
    "    else:\n",
    "        print(f\"Không tìm thấy bản ghi nào có độ dài vượt quá {max_length} tokens.\")\n",
    "\n",
    "# ... (phần code khác)\n",
    "\n",
    "def main():\n",
    "    # ... (phần code xử lý dữ liệu và tạo file JSON)\n",
    "\n",
    "    # Sau khi đã lưu dữ liệu ra file JSON\n",
    "    output_json_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\data.json\"\n",
    "    check_text_length(output_json_file) # kiểm tra độ dài của text\n",
    "\n",
    "    # ... (phần code upload lên Elasticsearch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chuyen finaldf sang json ma k bi loi dau ngoac don ngoac kep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu dữ liệu vào file JSON: D:\\DATN\\QA_System\\casestudy\\pdf\\finaldf0.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "\n",
    "execfile = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\finaldf0.xlsx\"\n",
    "df = pd.read_excel(execfile)\n",
    "\n",
    "# Sửa lỗi định dạng JSON trong cột metadata\n",
    "def fix_json_format(text):\n",
    "  \"\"\"\n",
    "  Sửa lỗi định dạng JSON bằng cách thay thế dấu ngoặc đơn bao quanh tên thuộc tính \n",
    "  bằng dấu ngoặc kép và đánh giá chuỗi như một dictionary.\n",
    "  \"\"\"\n",
    "  try:\n",
    "      # Thay thế dấu nháy đơn bao quanh tên thuộc tính bằng dấu nháy kép\n",
    "      # Tuy nhiên cách này sẽ thay thế tất cả dấu nháy đơn, bao gồm cả trong giá trị\n",
    "      # text = text.replace(\"'\", '\"') \n",
    "      \n",
    "      # Thay thế dấu nháy đơn bao quanh key, sử dụng ast.literal_eval để đánh giá chuỗi như một dictionary\n",
    "      \n",
    "      corrected_text = ast.literal_eval(text)\n",
    "      \n",
    "      # Chuyển đổi lại thành chuỗi JSON với định dạng chính xác\n",
    "      return json.dumps(corrected_text) \n",
    "  except (ValueError, SyntaxError):\n",
    "    return text\n",
    "    \n",
    "\n",
    "if isinstance(df[\"metadata\"].iloc[0], str):\n",
    "    df[\"metadata\"] = df[\"metadata\"].apply(fix_json_format)\n",
    "    df[\"metadata\"] = df[\"metadata\"].apply(lambda x: json.loads(x))\n",
    "\n",
    "# Chuyển đổi DataFrame thành danh sách các dictionary\n",
    "json_data = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Lưu danh sách các dictionary vào file JSON\n",
    "output_json_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\finaldf0.json\"\n",
    "with open(output_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Đã lưu dữ liệu vào file JSON: {output_json_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
