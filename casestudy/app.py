import streamlit as st
import logging
import time
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from retrieval import hybrid_retriever , qaretrieval
# Configure logging with UTF-8 encoding
logging.basicConfig(
    level=logging.INFO,
    #format='%(asctime)s - %(levelname)s - %(message)s',
    format = '%(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
from model_config import load_gpt4o_mini_model , load_gemini15, load_together_model , load_groq_model
from prompt import QA_TEMPLATE, REWRITE_TEMPLATE
COLLECTION_NAME = "raptor"
BASE_COLLECTION_NAME = "base"
QA_COLLECTION_NAME = "faq_data"
THRESHOLD = 0.85
# Khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n global
load_dotenv()
st.set_page_config(page_title="University Assistant", page_icon="ğŸ¤–")
st.title("University Assistant")
@st.cache_resource
def initialize_db():
    logger.info("Khá»Ÿi táº¡o database vÃ  FAQ...")
    db = hybrid_retriever(COLLECTION_NAME)
    qa_db = qaretrieval(QA_COLLECTION_NAME)
    return db, qa_db

@st.cache_resource
def initialize_model(selected_model):
    logger.info(f"Khá»Ÿi táº¡o model {selected_model}...")
    
    if selected_model == "Together_ai.meta-llama/Llama-3.3-70B-Instruct-Turbo":
        llm = load_together_model()
    elif selected_model == "Groq_llama-3.3-70b-versatile":
        llm = load_groq_model()
    # elif selected_model == "GPT4o Mini":
    #     llm = load_gpt4o_mini_model()
    elif selected_model == "Gemini1.5Flash":
        llm = load_gemini15()
    else:
        raise ValueError("Model khÃ´ng há»£p lá»‡!")
    return llm
db, qa_db = initialize_db()

# ThÃªm giao diá»‡n chá»n model
selected_model = st.sidebar.selectbox(
    "Chá»n model",
    [ "Together_ai.meta-llama/Llama-3.3-70B-Instruct-Turbo", "Groq_llama-3.3-70b-versatile","Gemini1.5Flash"]
    )

# Khá»Ÿi táº¡o model dá»±a trÃªn lá»±a chá»n
llm = initialize_model(selected_model)



def rewrite_question(user_query, formatted_history, llm):  # ThÃªm tham sá»‘ `llm`
    logger.info("=== Báº¯t Ä‘áº§u viáº¿t láº¡i cÃ¢u há»i ===")
    #logger.info(f"CÃ¢u há»i gá»‘c: {user_query}")
    
    if formatted_history:
        logger.info("Äang sá»­ dá»¥ng lá»‹ch sá»­ chat Ä‘á»ƒ viáº¿t láº¡i cÃ¢u há»i")
        #logger.info(f"Lá»‹ch sá»­ chat Ä‘Ã£ format: {formatted_history}")
    else:
        logger.info("KhÃ´ng cÃ³ lá»‹ch sá»­ chat trÆ°á»›c Ä‘Ã³")
    
    rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_TEMPLATE)
    chain = rewrite_prompt | llm | StrOutputParser()  # Sá»­ dá»¥ng `llm` truyá»n vÃ o thay vÃ¬ toÃ n cá»¥c
    
    logger.info("Äang gá»­i yÃªu cáº§u viáº¿t láº¡i cÃ¢u há»i tá»›i model...")
    rewritten_query = chain.invoke({
        "chat_history": formatted_history,
        "question": user_query
    })
    logger.info(f"CÃ¢u há»i gá»‘c: {user_query}")
    logger.info(f"CÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c viáº¿t láº¡i: {rewritten_query.strip()}")
    logger.info("=== Káº¿t thÃºc viáº¿t láº¡i cÃ¢u há»i ===")
    return rewritten_query.strip()


def format_chat_history(chat_history, max_pairs=5):
    formatted_history = []
    qa_pairs = []

    for i in range(len(chat_history)-1):
        if isinstance(chat_history[i], HumanMessage) and isinstance(chat_history[i+1], AIMessage):
            qa_pairs.append({
                "question": chat_history[i].content,
                "answer": chat_history[i+1].content
            })

    recent_pairs = qa_pairs[-max_pairs:] if qa_pairs else []
    
    for pair in recent_pairs:
        formatted_history.append(f"Human: {pair['question']}\nAssistant: {pair['answer']}")

    return "\n\n".join(formatted_history)

def check_faq(user_query, qa_db):  # Loáº¡i bá» giÃ¡ trá»‹ máº·c Ä‘á»‹nh, `qa_db` lÃ  tham sá»‘ báº¯t buá»™c
    logger.info("Truy váº¥n DB cho FAQ...")
    # Gá»i `semantic_qa_search` Ä‘á»ƒ tÃ¬m kiáº¿m trong collection
    result = qa_db.invoke(user_query)  # Sá»­ dá»¥ng `qa_db` truyá»n vÃ o thay vÃ¬ toÃ n cá»¥c
    
    if len(result) > 0:
        logger.info(f"CÃ¢u há»i phÃ¹ há»£p tÃ¬m tháº¥y trong QADB. Score = {result[0].metadata.get('_score')}")
        return result[0].page_content  # Tráº£ vá» cÃ¢u tráº£ lá»i tá»‘t nháº¥t

    logger.info("KhÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i phÃ¹ há»£p trong QADB.")
    return None


def stream_text(text):
    """Generator function Ä‘á»ƒ stream tá»«ng tá»« cá»§a vÄƒn báº£n"""
    time.sleep(1)
    # Chuá»—i má»›i Ä‘Æ°á»£c thÃªm vÃ o cuá»‘i vÄƒn báº£n
    new_text =text +" Náº¿u báº¡n cÃ³ cÃ¢u há»i nÃ o khÃ¡c hÃ£y cho tÃ´i biáº¿t nhÃ©! Má»i báº¡n tham kháº£o thÃªm cÃ¡c thÃ´ng tin khÃ¡c táº¡i Ä‘Ã¢y: "
    link = "[Sá»• tay sinh viÃªn](https://ctsv.hust.edu.vn/#/so-tay-sv)"
    # Streaming tá»«ng tá»« trong vÄƒn báº£n
    for word in new_text.split():
        yield word + " "
        # Náº¿u muá»‘n Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ stream, cÃ³ thá»ƒ thÃªm time.sleep()
        time.sleep(0.05)
    yield link
    # Sau khi hoÃ n thÃ nh streaming, hiá»ƒn thá»‹ link
    #st.markdown(link)

def get_response(user_query, chat_history, db, qa_db, llm):  # ThÃªm `qa_db` vÃ  `llm` lÃ m tham sá»‘
    logger.info("=== Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh táº¡o cÃ¢u tráº£ lá»i ===")

    # Kiá»ƒm tra trong FAQ
    answer = check_faq(user_query, qa_db)  # Truyá»n `qa_db` vÃ o
    if answer:
        return stream_text(answer)
    # KhÃ´ng tÃ¬m tháº¥y, gá»i Ä‘áº¿n RAG
    logger.info("KhÃ´ng tÃ¬m tháº¥y cÃ¢u há»i phÃ¹ há»£p trong FAQ, chuyá»ƒn Ä‘áº¿n há»‡ thá»‘ng RAG.")
    # Viáº¿t láº¡i cÃ¢u há»i
    logger.info("=== Báº¯t Ä‘áº§u format chat history ===")
    formatted_history = format_chat_history(chat_history, max_pairs=5)
    rewritten_query = rewrite_question(user_query, formatted_history, llm)  # Truyá»n `llm` vÃ o
    if "<spam>" in rewritten_query:
        default_response = "Xin lá»—i, tÃ´i chá»‰ lÃ  trá»£ lÃ½ áº£o cá»§a Äáº¡i há»c vÃ  tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y. "
        logger.info("PhÃ¡t hiá»‡n cÃ¢u há»i khÃ´ng liÃªn quan, tráº£ lá»i máº·c Ä‘á»‹nh.")
        return stream_text(default_response)
    # Truy váº¥n thÃ´ng tin liÃªn quan  
    logger.info("=== Báº¯t Ä‘áº§u truy xuáº¥t tÃ i liá»‡u ===")
    relevant_docs = db.invoke(rewritten_query)
    logger.info(f"Sá»‘ lÆ°á»£ng tÃ i liá»‡u tÃ¬m Ä‘Æ°á»£c: {len(relevant_docs)}")

    logger.info("=== Káº¿t thÃºc truy xuáº¥t tÃ i liá»‡u ===")
    doc_context = "\n\n".join([doc.page_content for doc in relevant_docs])
    logger.info(f"Äá»™ dÃ i context: {len(doc_context)} kÃ½ tá»±")
    logger.info("=== Káº¿t thÃºc format dá»¯ liá»‡u ===")

    QAprompt = ChatPromptTemplate.from_template(QA_TEMPLATE)
    logger.info("=== Báº¯t Ä‘áº§u táº¡o cÃ¢u tráº£ lá»i ===")
    logger.info(f"=== full QA_prompt {QAprompt}===")
    
    chain = QAprompt | llm | StrOutputParser()  # Sá»­ dá»¥ng `llm` truyá»n vÃ o thay vÃ¬ toÃ n cá»¥c
    logger.info("Äang gá»­i yÃªu cáº§u tá»›i model...")
    response = chain.stream({
        "chat_history": formatted_history,
        "documents": doc_context,
        "question": rewritten_query
    })
    logger.info("=== Káº¿t thÃºc táº¡o cÃ¢u tráº£ lá»i ===")
    logger.info(f"CÃ¢u tráº£ lá»i: {response}")
    return response


###app###
def process_user_input(user_query):
    if not user_query:
        return None
    if len(user_query) > 512:
        st.error("Äá»™ dÃ i cÃ¢u há»i quÃ¡ dÃ i, vui lÃ²ng nháº­p cÃ¢u há»i ngáº¯n hÆ¡n!")
        return None
    logger.info(f"\n=== Báº¯t Ä‘áº§u xá»­ lÃ½ cÃ¢u há»i má»›i:{user_query} ===\n")
    st.session_state.chat_history.append(HumanMessage(content=user_query))
    
    with st.chat_message("Human"):
        st.markdown(user_query)
    
    with st.chat_message("AI"):
        response = st.write_stream(get_response(user_query, st.session_state.chat_history, db, qa_db, llm))

        logger.info(f"CÃ¢u tráº£ lá»i cho ngÆ°á»i dÃ¹ng: {response}===\n")

    st.session_state.chat_history.append(AIMessage(content=response))
    return response
def display_chat_history():
    if "chat_history" not in st.session_state:
        logger.info("=== Khá»Ÿi táº¡o phiÃªn chat má»›i ===\n")
        st.session_state.chat_history = []
        st.session_state.chat_history.append(AIMessage(
            content="ChÃ o báº¡n, TÃ´i lÃ  trá»£ lÃ½ áº£o cá»§a Äáº¡i há»c . TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n?"
        ))
    
    for message in st.session_state.chat_history:
        if isinstance(message, AIMessage):
            with st.chat_message("AI"):
                st.write(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.write(message.content)

# Main interaction
display_chat_history()
user_query = st.chat_input("Nháº­p tin nháº¯n cá»§a báº¡n...")
if user_query:
    process_user_input(user_query)